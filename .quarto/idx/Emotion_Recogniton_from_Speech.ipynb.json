{"title":"<b>Emotion Recognition from Speech</b>","markdown":{"yaml":{"title":"<b>Emotion Recognition from Speech</b>","format":{"html":{"theme":"lumen","toc":true,"self-contained":true,"embed-resources":true,"page-layout":"full","code-fold":true,"code-tools":true}},"jupyter":"python3"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nSpeech Emotion Recognition (SER) is a field of study in Artificial Intelligence (AI) that aims to automatically recognize human emotions through speech. The ability to understand human emotions can be beneficial in many domains, such as psychotherapy, customer service, and human-robot interaction. The RAVDESS dataset is a popular database for speech emotion recognition research. It contains audio and visual recordings of actors performing short clips of emotional speech. The dataset is labeled with seven different emotions: neutral, calm, happy, sad, angry, fearful, and disgust. In this project, we will explore deep learning techniques to build a SER model using the `RAVDESS` dataset.\n\nDeep learning models have become the state-of-the-art approach in SER because of their ability to learn complex patterns in speech signals. Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) are two popular deep learning models used for SER. CNNs are particularly suitable for speech analysis because they can automatically learn relevant features from the spectrogram of the audio signal. RNNs, on the other hand, can capture the temporal information of the audio signal by modeling the sequence of the acoustic features.\n\nThe RAVDESS dataset has been widely used in the research community to develop and evaluate deep learning models for SER. In this project, we will focus on building a CNN-based model for speech emotion recognition. We will preprocess the audio files to extract `Mel-frequency cepstral coefficients (MFCCs)` and `Chroma` features. These features will be used to create spectrograms, which are two-dimensional representations of the audio signals. The spectrograms will be fed into a CNN to train a model for speech emotion recognition. We will evaluate the model's performance using accuracy, precision, recall, and F1-score metrics.\n\nThe remainder of this project is organized as follows. In the next section, we will discuss the RAVDESS dataset in detail, including its characteristics, structure, and contents. We will also discuss the preprocessing steps required to prepare the dataset for deep learning models. In the following section, we will describe the CNN architecture used for SER and the training process. We will also present the evaluation metrics used to measure the performance of the model. Finally, we will present the results of the experiments and discuss the limitations and potential improvements of the model.\n\n# Import Libraries\n\n# Preparing the Data\n\n## Plotting different emotion audio's waveform and its spectrogram\n\nAnger:\n\nFear:\n\nHappy:\n\nThe shape of spectrograms, which represent the frequency content of an audio signal over time, can vary depending on the characteristics of the audio signal. In the context of emotion recognition, different emotions may be associated with different patterns of spectral energy distribution and temporal dynamics in the audio signal. For example, angry or fearful speech may be characterized by higher spectral energy in the high-frequency range and shorter temporal duration, while happy or neutral speech may be characterized by lower spectral energy and longer duration. Machine learning models can be trained to automatically extract these patterns and recognize the corresponding emotions based on the spectrogram features.\n\n## Functions for converting our data into CSV\n\nThe `create_meta_csv` function creates a CSV file that contains the file paths and corresponding labels of WAV files in a dataset directory. The function first retrieves the absolute path of the dataset directory and sets the path for the CSV file to be created. It then loops through the dataset directory and creates a list of tuples, where each tuple contains the file path and its corresponding label. Finally, the function opens the CSV file and writes the header row and data rows from the list of tuples. If the destination path is None, it is set to the dataset path. The function returns True to indicate that it has completed successfully.\n\nThe `create_and_load_meta_csv_df` function creates and loads a CSV file containing metadata about an audio dataset located at `dataset_path`. The function also provides options to shuffle the rows of the dataset randomly and split the dataset into training and testing sets. If `randomize` is True or `split` is not None and `randomize` is None, the function shuffles the rows of the DataFrame. If `split` is not None, the function splits the DataFrame into training and testing sets. The function returns the DataFrame or all three DataFrames, including training and testing sets, depending on the options selected. The `create_meta_csv` function is called to create the CSV file containing the metadata, which is then loaded into a Pandas DataFrame.\n\nThe `train_test_split` function takes in a DataFrame `dframe` and a split ratio `split_ratio`, which determines the proportion of data to be used for training and testing. It splits the DataFrame into two separate DataFrames: the training set, which contains `split_ratio` percent of the data, and the testing set, which contains the remaining data. The function then resets the index of the testing set to start at 0, and returns the two DataFrames as separate objects. This function is commonly used in machine learning to split a dataset into training and testing sets in order to evaluate the performance of a model on new, unseen data.\n\nRunning all the above functions:\n\nSeeing a sample of the training data:\n\n# Visualizing the Data\n\n\nLabels Assigned for emotions : \n- 0 : anger\n- 1 : disgust\n- 2 : fear\n- 3 : happy\n- 4 : neutral \n- 5 : sad\n- 6 : surprise\n\n\n## Counting the number of emotions\n\nThe histogram shows the distribution of the counts of unique labels in the Emotion dataset. The x-axis shows the label numbers and the y-axis shows the count of each label. From the histogram, we can see that the dataset is fairly balanced, with each label having a similar count. Labels 0, 2, 3, 5 have slightly higher counts than the other labels, but the difference is not significant. This means that the model trained on this dataset should be able to generalize well to different emotions, as it has a good representation of all the emotions in the dataset.\n\n# Pre-Processing the Data\n\nAudio Features:\n\n- `Mel Frequency Cepstral Coefficients (MFCC)` : It is a feature extraction technique widely used in speech processing and recognition. It involves extracting the spectral envelope of a speech signal, typically using the Discrete Fourier Transform (DFT), and then mapping it to the mel frequency scale, which better reflects human perception of sound. From this mel-scaled spectrogram, the MFCCs are obtained by taking the logarithm of the power spectrum and performing a discrete cosine transform. The resulting MFCCs capture the most relevant information of the speech signal, such as phonetic content, speaker identity, and emotion. MFCCs are commonly used as inputs to machine learning algorithms for speech recognition and related tasks.\n- `Chroma` feature extraction: It is a technique used to represent the harmonic content of an audio signal in a compact manner. Chroma features are based on the pitch class profiles of musical notes, which are invariant to octave transposition and are typically represented using a circular layout called the chroma circle. Chroma features can be computed from the short-term Fourier transform (STFT) of an audio signal, by first mapping the power spectrum to the pitch class domain and then summing the energy of each pitch class over time. The resulting chroma feature matrix can be used as input to machine learning algorithms for tasks such as music genre classification, chord recognition, and melody extraction.\n- `Pitch`: It is a perceptual attribute of sound that allows us to distinguish between high and low frequency sounds. It is closely related to the physical property of frequency, which is the number of cycles per second that a sound wave completes. High-pitched sounds have a high frequency, while low-pitched sounds have a low frequency. In music, pitch is used to describe the perceived height or lowness of a musical note. Pitch can be manipulated by altering the frequency of a sound wave using techniques such as tuning or modulation. Pitch perception is an important aspect of human auditory processing and is essential for tasks such as speech recognition and music appreciation.\n- `Magnitude`: In signal processing, magnitude refers to the amplitude or strength of a signal, which is a measure of how much energy is contained in the signal. It is typically calculated as the absolute value of a complex number, which is a mathematical representation of a signal that includes both its magnitude and phase. Magnitude can be used to describe various characteristics of a signal, such as its power, energy, or intensity. For example, in the context of audio signal processing, magnitude can be used to represent the loudness or volume of a sound, while in image processing, magnitude can be used to represent the strength of different frequencies in an image.\n\n## Functions for getting features of audio files\n\nThe function `get_audio_features` extracts audio features from a given audio file path using the Librosa library in Python. It loads the audio file at the specified path, resamples it to a target rate, and separates the harmonic and percussive components of the audio signal. It then computes the pitch and magnitude of the audio signal using the PIPtrack algorithm and extracts the 20 most prominent values of each. The function also computes the Mel-frequency cepstral coefficients (MFCCs) from the audio signal and the chroma feature from the harmonic component using the Constant-Q Transform (CQT). Finally, it returns a list of features including the MFCCs, pitch, magnitude, and chroma features. The output of this function is used as input to train machine learning models for audio classification tasks.\n\nThe function `get_features_dataframe` takes a Pandas dataframe containing audio file paths and their corresponding labels, as well as a sampling rate as input. It uses the `get_audio_features` function to extract audio features for each file path in the dataframe, and stores these features in a new Pandas dataframe. The resulting dataframe is split into separate dataframes for each type of feature (i.e. mfcc, pitches, magnitudes, and C), which are then concatenated into a single dataframe. The function returns this combined feature dataframe along with a separate dataframe containing the original labels.\n\nGetting the features of audio files using librosa (Usually takes 12-15 mins to run):\n\n\n\nFill NA values with 0:\n\nConverting 2D to 1D using .ravel():\n\nOne-Hot Encoding the labels:\n\nChanging dimension for Neural Networks model:\n\n# Deep Learning Model\n\n## Creating a Model\n\nSummary of the Sequential Model:\n\nCompile the model:\n\n## Training and Evaluation\n\nTraining the model:\n\nPlotting Loss Vs Iterations:\n\n## Saving the model\n\nSaving .h5 model file:\n\nSaving .json model file:\n\n## Loading the model\n\n# Test Set Prediction\n\n### Predicting emotions on the test data\n\n## Actual v/s Predicted emotions\n\nSaving the predicted values in a CSV file:\n\n# Demonstration on a demo audio file\n\nLoading the demo audio file:\n\nGetting the features of the demo audio file:\n\nPredicting the emotion of the demo audio file:\n\n# Conclusions\n\nSpeech Emotion Recognition (SER) is an important research area in the field of signal processing and machine learning. In this project, we used the Ravdess dataset, which contains a diverse range of emotional speech recordings. Our goal was to train a deep learning model that can accurately classify the emotional state of a speaker from their speech signal. We used a combination of signal processing techniques such as MFCC, Chroma, Pitch and Magnitude to extract features from the audio signals.\n\nWe preprocessed the data by normalizing the amplitude of the signals and segmenting them into fixed-length frames. We then trained a Neural Network using the Keras API in TensorFlow to classify the emotion from the audio signal. The model architecture consisted of multiple layers of Conv1D and MaxPooling1D followed by a Flatten layer and a Dense layer with a RMSprop activation function. We used the RMSprop optimization technique to minimize the loss function and the accuracy metric to evaluate the model's performance.\n\nOur results show that the model achieved a test accuracy of 70%, which is a promising result considering the complexity of the task and the size of the dataset. We also observed that the model performed better on certain emotions, such as neutral and happy, while struggling to accurately classify others, such as disgust and surprise. This could be due to the fact that some emotions are more distinct and easily recognizable from the audio signal than others.\n\nIn conclusion, our study demonstrates the feasibility of using deep learning models for speech emotion recognition tasks. The Ravdess dataset provides a rich resource for future research in this area, with its diverse range of emotions and large number of samples. However, our study also highlights some of the challenges and limitations of this approach, such as the need for large amounts of data and the difficulty of accurately classifying certain emotions. Future work could explore alternative feature extraction techniques and model architectures to further improve the performance of speech emotion recognition systems. Additionally, the development of real-world applications for this technology could have a significant impact on fields such as psychology, human-computer interaction, and entertainment.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"self-contained":true,"embed-resources":true,"output-file":"Emotion_Recogniton_from_Speech.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","theme":"cosmo","title":"<b>Emotion Recognition from Speech</b>","jupyter":"python3","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}