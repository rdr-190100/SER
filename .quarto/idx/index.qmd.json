{"title":"<b>Emotion Recognition from Speech</b>","markdown":{"yaml":{"title":"<b>Emotion Recognition from Speech</b>","format":{"html":{"theme":"lumen","toc":true,"self-contained":true,"embed-resources":true,"page-layout":"full","code-fold":true,"code-tools":true}},"jupyter":"python3"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nSpeech Emotion Recognition (SER) is a field of study in Artificial Intelligence (AI) that aims to automatically recognize human emotions through speech. The ability to understand human emotions can be beneficial in many domains, such as psychotherapy, customer service, and human-robot interaction. The RAVDESS dataset is a popular database for speech emotion recognition research. It contains audio and visual recordings of actors performing short clips of emotional speech. The dataset is labeled with seven different emotions: neutral, calm, happy, sad, angry, fearful, and disgust. In this project, we will explore deep learning techniques to build a SER model using the `RAVDESS` dataset.\n\nDeep learning models have become the state-of-the-art approach in SER because of their ability to learn complex patterns in speech signals. Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) are two popular deep learning models used for SER. CNNs are particularly suitable for speech analysis because they can automatically learn relevant features from the spectrogram of the audio signal. RNNs, on the other hand, can capture the temporal information of the audio signal by modeling the sequence of the acoustic features.\n\nThe RAVDESS dataset has been widely used in the research community to develop and evaluate deep learning models for SER. In this project, we will focus on building a CNN-based model for speech emotion recognition. We will preprocess the audio files to extract `Mel-frequency cepstral coefficients (MFCCs)` and `Chroma` features. These features will be used to create spectrograms, which are two-dimensional representations of the audio signals. The spectrograms will be fed into a CNN to train a model for speech emotion recognition. We will evaluate the model's performance using accuracy, precision, recall, and F1-score metrics.\n\nThe remainder of this project is organized as follows. In the next section, we will discuss the RAVDESS dataset in detail, including its characteristics, structure, and contents. We will also discuss the preprocessing steps required to prepare the dataset for deep learning models. In the following section, we will describe the CNN architecture used for SER and the training process. We will also present the evaluation metrics used to measure the performance of the model. Finally, we will present the results of the experiments and discuss the limitations and potential improvements of the model.\n\n# Import Libraries\n\n```{python}\nimport matplotlib.pyplot as plt\nimport os\nimport librosa\nimport librosa.display\n\nimport IPython.display as ipd\nfrom IPython.display import Image\n\nimport seaborn as sns\nsns.set_palette('Set2')\nfrom librosa.core import pitch\n\nimport numpy as np\nimport pandas as pd\n\nimport keras\nimport tensorflow\nfrom tensorflow.keras import optimizers\nfrom tensorflow.python.keras import Sequential\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D #, AveragePooling1D\nfrom keras.layers import Flatten, Dropout, Activation # Input, \nfrom keras.layers import Dense #, Embedding\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\n\nimport wave\nimport sys\nimport csv\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\n\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n# Preparing the Data\n\n## Plotting different emotion audio's waveform and its spectrogram\n\nAnger:\n\n```{python}\ndata, sampling_rate = librosa.load('Dataset/anger/anger001.wav')\nipd.Audio('Dataset/anger/anger016.wav')\n```\n\n```{python}\nplt.figure(figsize=(12, 4))\nlibrosa.display.waveshow(data, sr=sampling_rate, alpha=0.4)\nplt.title('Spectrogram for Anger Audio Sample')\nplt.xlabel('Time')\nplt.ylabel('Frequency')\nplt.show()\n```\n\nFear:\n\n```{python}\ndata, sampling_rate = librosa.load('Dataset/fear/fear001.wav')\nipd.Audio('Dataset/fear/fear001.wav')\n```\n\n```{python}\nplt.figure(figsize=(12, 4))\nlibrosa.display.waveshow(data, sr=sampling_rate, alpha=0.4)\nplt.title('Spectrogram for Fear Audio Sample')\nplt.xlabel('Time')\nplt.ylabel('Frequency')\nplt.show()\n```\n\nHappy:\n\n```{python}\ndata, sampling_rate = librosa.load('Dataset/happy/happy002.wav')\nipd.Audio('Dataset/happy/happy002.wav')\n```\n\n```{python}\nplt.figure(figsize=(12, 4))\nlibrosa.display.waveshow(data, sr=sampling_rate, alpha=0.4)\nplt.title('Spectrogram for Happy Audio Sample')\nplt.xlabel('Time')\nplt.ylabel('Frequency')\nplt.show()\n```\n\nThe shape of spectrograms, which represent the frequency content of an audio signal over time, can vary depending on the characteristics of the audio signal. In the context of emotion recognition, different emotions may be associated with different patterns of spectral energy distribution and temporal dynamics in the audio signal. For example, angry or fearful speech may be characterized by higher spectral energy in the high-frequency range and shorter temporal duration, while happy or neutral speech may be characterized by lower spectral energy and longer duration. Machine learning models can be trained to automatically extract these patterns and recognize the corresponding emotions based on the spectrogram features.\n\n## Functions for converting our data into CSV\n\nThe `create_meta_csv` function creates a CSV file that contains the file paths and corresponding labels of WAV files in a dataset directory. The function first retrieves the absolute path of the dataset directory and sets the path for the CSV file to be created. It then loops through the dataset directory and creates a list of tuples, where each tuple contains the file path and its corresponding label. Finally, the function opens the CSV file and writes the header row and data rows from the list of tuples. If the destination path is None, it is set to the dataset path. The function returns True to indicate that it has completed successfully.\n\n```{python}\nnp.random.seed(42)\n\ndef create_meta_csv(dataset_path, destination_path):\n\n    # Get the absolute path of the dataset directory\n    DATASET_PATH = os.path.abspath(dataset_path)\n\n    # Set the path of the CSV file that will be created\n    csv_path=os.path.join(destination_path, 'dataset_attr.csv')\n\n    # Create an empty list to hold the file paths of all the WAV files\n    flist = []\n\n    # Define a list of emotions that will be used as labels in the CSV file\n    emotions=[\"anger\",\"disgust\",\"fear\",\"happy\",\"neutral\", \"sad\", \"surprise\"]\n\n    # Loop through the dataset directory and add the file paths of all the WAV files to the flist list\n    for root, dirs, files in os.walk(DATASET_PATH, topdown=False):\n        for name in files:\n            if (name.endswith('.wav')): \n                fullName = os.path.join(root, name)\n                flist.append(fullName)\n\n    # Split each file path in flist by the directory separator and store the result in a new list called filenames\n    filenames=[]\n    for idx,file in enumerate(flist):\n        filenames.append(file.split('/')) \n\n    # Create a list of tuples, where each tuple contains a file path and its corresponding emotion label\n    types=[]\n    for idx,path in enumerate(filenames):\n        types.append((flist[idx],emotions.index(path[-2])))\n\n    # Open the CSV file, write the header row, and write the data rows from the types list\n    with open(csv_path, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerows([(\"path\",\"label\")])\n        writer.writerows(types)\n    f.close()\n\n    # If the destination path is None, set it to the dataset path\n    if destination_path == None:\n        destination_path = DATASET_PATH\n\n    # Return True to indicate that the function has completed successfully\n    return True\n```\n\nThe `create_and_load_meta_csv_df` function creates and loads a CSV file containing metadata about an audio dataset located at `dataset_path`. The function also provides options to shuffle the rows of the dataset randomly and split the dataset into training and testing sets. If `randomize` is True or `split` is not None and `randomize` is None, the function shuffles the rows of the DataFrame. If `split` is not None, the function splits the DataFrame into training and testing sets. The function returns the DataFrame or all three DataFrames, including training and testing sets, depending on the options selected. The `create_meta_csv` function is called to create the CSV file containing the metadata, which is then loaded into a Pandas DataFrame.\n\n```{python}\ndef create_and_load_meta_csv_df(dataset_path, destination_path, randomize=True, split=None):\n   \n    # Call create_meta_csv to generate the CSV file containing metadata about the audio dataset\n    if create_meta_csv(dataset_path, destination_path=destination_path):\n        # Load the CSV file into a Pandas DataFrame\n        dframe = pd.read_csv(os.path.join(destination_path, 'dataset_attr.csv'))\n\n    # If randomize is True or split is not None and randomize is None, shuffle the rows of the DataFrame\n    if randomize == True or (split != None and randomize == None):\n        dframe=dframe.sample(frac=1).reset_index(drop=True)\n        pass\n\n    # If split is not None, split the DataFrame into training and testing sets\n    if split != None:\n        train_set, test_set = train_test_split(dframe, split)\n        return dframe, train_set, test_set \n    \n    # Return the DataFrame\n    return dframe\n```\n\nThe `train_test_split` function takes in a DataFrame `dframe` and a split ratio `split_ratio`, which determines the proportion of data to be used for training and testing. It splits the DataFrame into two separate DataFrames: the training set, which contains `split_ratio` percent of the data, and the testing set, which contains the remaining data. The function then resets the index of the testing set to start at 0, and returns the two DataFrames as separate objects. This function is commonly used in machine learning to split a dataset into training and testing sets in order to evaluate the performance of a model on new, unseen data.\n\n```{python}\ndef train_test_split(dframe, split_ratio):\n\n    # Split the DataFrame into training and testing sets based on the split ratio\n    train_data= dframe.iloc[:int((split_ratio) * len(dframe)), :]\n    test_data= dframe.iloc[int((split_ratio) * len(dframe)):,:]\n    \n    # Reset the index of the testing set to start at 0\n    test_data=test_data.reset_index(drop=True) \n    \n    # Return the training and testing sets as separate DataFrames\n    return train_data, test_data\n```\n\nRunning all the above functions:\n\n```{python}\n# Set the path to the audio dataset directory and print it\ndataset_path =  './Dataset'\nprint(\"dataset_path : \", dataset_path)\n\n# Set the destination path to the current working directory\ndestination_path = os.getcwd()\n\n# Set the number of classes in the audio dataset\nclasses = 7\n\n# Set the total number of rows in the audio dataset\ntotal_rows = 2556\n\n# Set the randomize and clear flags to True\nrandomize = True\nclear = True\n\n# Create and load the metadata CSV file for the audio dataset, and split it into training and testing sets\ndf, train_df, test_df = create_and_load_meta_csv_df(dataset_path, destination_path=destination_path, randomize=randomize, split=0.99)\n```\n\nSeeing a sample of the training data:\n\n```{python}\ntrain_df.head()\n```\n\n# Visualizing the Data\n\nLabels Assigned for emotions : \n- 0 : anger\n- 1 : disgust\n- 2 : fear\n- 3 : happy\n- 4 : neutral \n- 5 : sad\n- 6 : surprise\n\n## Counting the number of emotions\n\n```{python}\n# Get the unique labels in the training set of the Emotion dataset and print them in sorted order\nunique_labels = train_df.label.unique()\nunique_labels.sort()\nprint(\"Unique labels in Emotion dataset : \")\nprint(*unique_labels, sep=', ')\n\n# Get the count of each unique label in the training set of the Emotion dataset and print them\nunique_labels_counts = train_df.label.value_counts(sort=False)\nprint(\"\\nCount of unique labels in Emotion dataset : \")\nprint(*unique_labels_counts,sep=', ')\n```\n\n```{python}\n# Histogram of the labels\nplt.figure(figsize=(10, 5))\nsns.countplot(x='label', data=train_df)\nplt.xticks(unique_labels)\nplt.ylabel('Count')\nplt.xlabel('Emotion')\n_ = plt.title('Distribution of Emotion in Data')\nplt.show()\n```\n\nThe histogram shows the distribution of the counts of unique labels in the Emotion dataset. The x-axis shows the label numbers and the y-axis shows the count of each label. From the histogram, we can see that the dataset is fairly balanced, with each label having a similar count. Labels 0, 2, 3, 5 have slightly higher counts than the other labels, but the difference is not significant. This means that the model trained on this dataset should be able to generalize well to different emotions, as it has a good representation of all the emotions in the dataset.\n\n# Pre-Processing the Data\n\nAudio Features:\n\n- `Mel Frequency Cepstral Coefficients (MFCC)` : It is a feature extraction technique widely used in speech processing and recognition. It involves extracting the spectral envelope of a speech signal, typically using the Discrete Fourier Transform (DFT), and then mapping it to the mel frequency scale, which better reflects human perception of sound. From this mel-scaled spectrogram, the MFCCs are obtained by taking the logarithm of the power spectrum and performing a discrete cosine transform. The resulting MFCCs capture the most relevant information of the speech signal, such as phonetic content, speaker identity, and emotion. MFCCs are commonly used as inputs to machine learning algorithms for speech recognition and related tasks.\n- `Chroma` feature extraction: It is a technique used to represent the harmonic content of an audio signal in a compact manner. Chroma features are based on the pitch class profiles of musical notes, which are invariant to octave transposition and are typically represented using a circular layout called the chroma circle. Chroma features can be computed from the short-term Fourier transform (STFT) of an audio signal, by first mapping the power spectrum to the pitch class domain and then summing the energy of each pitch class over time. The resulting chroma feature matrix can be used as input to machine learning algorithms for tasks such as music genre classification, chord recognition, and melody extraction.\n- `Pitch`: It is a perceptual attribute of sound that allows us to distinguish between high and low frequency sounds. It is closely related to the physical property of frequency, which is the number of cycles per second that a sound wave completes. High-pitched sounds have a high frequency, while low-pitched sounds have a low frequency. In music, pitch is used to describe the perceived height or lowness of a musical note. Pitch can be manipulated by altering the frequency of a sound wave using techniques such as tuning or modulation. Pitch perception is an important aspect of human auditory processing and is essential for tasks such as speech recognition and music appreciation.\n- `Magnitude`: In signal processing, magnitude refers to the amplitude or strength of a signal, which is a measure of how much energy is contained in the signal. It is typically calculated as the absolute value of a complex number, which is a mathematical representation of a signal that includes both its magnitude and phase. Magnitude can be used to describe various characteristics of a signal, such as its power, energy, or intensity. For example, in the context of audio signal processing, magnitude can be used to represent the loudness or volume of a sound, while in image processing, magnitude can be used to represent the strength of different frequencies in an image.\n\n## Functions for getting features of audio files\n\nThe function `get_audio_features` extracts audio features from a given audio file path using the Librosa library in Python. It loads the audio file at the specified path, resamples it to a target rate, and separates the harmonic and percussive components of the audio signal. It then computes the pitch and magnitude of the audio signal using the PIPtrack algorithm and extracts the 20 most prominent values of each. The function also computes the Mel-frequency cepstral coefficients (MFCCs) from the audio signal and the chroma feature from the harmonic component using the Constant-Q Transform (CQT). Finally, it returns a list of features including the MFCCs, pitch, magnitude, and chroma features. The output of this function is used as input to train machine learning models for audio classification tasks.\n\n```{python}\ndef get_audio_features(audio_path,sampling_rate):\n    \n    # Load audio file at given path, resample to target rate, and extract features\n    X, sample_rate = librosa.load(audio_path, res_type='kaiser_fast', duration=2.5, sr=sampling_rate*2, offset=0.5)\n\n    # Convert sample rate to a NumPy array for consistency\n    sample_rate = np.array(sample_rate)\n\n    # Separate harmonic and percussive components of audio signal\n    y_harmonic, y_percussive = librosa.effects.hpss(X)\n\n    # Compute pitch and magnitude of audio signal using PIPtrack algorithm\n    pitches, magnitudes = librosa.core.pitch.piptrack(y=X, sr=sample_rate)\n\n    # Compute Mel-frequency cepstral coefficients (MFCCs) from audio signal\n    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=1)\n\n    # Extract the 20 most prominent pitch values from the pitch array\n    pitches = np.trim_zeros(np.mean(pitches, axis=1))[:20]\n\n    # Extract the 20 most prominent magnitude values from the magnitude array\n    magnitudes = np.trim_zeros(np.mean(magnitudes, axis=1))[:20]\n\n    # Compute chroma feature from harmonic component of audio signal using Constant-Q Transform (CQT)\n    C = np.mean(librosa.feature.chroma_cqt(y=y_harmonic, sr=sampling_rate), axis=1)\n\n    # Return a list of audio features, including MFCCs, pitch, magnitude, and chroma features\n    return [mfccs, pitches, magnitudes, C]\n```\n\nThe function `get_features_dataframe` takes a Pandas dataframe containing audio file paths and their corresponding labels, as well as a sampling rate as input. It uses the `get_audio_features` function to extract audio features for each file path in the dataframe, and stores these features in a new Pandas dataframe. The resulting dataframe is split into separate dataframes for each type of feature (i.e. mfcc, pitches, magnitudes, and C), which are then concatenated into a single dataframe. The function returns this combined feature dataframe along with a separate dataframe containing the original labels.\n\n```{python}\ndef get_features_dataframe(dataframe, sampling_rate):\n    \n    # Create a new dataframe to hold the labels\n    labels = pd.DataFrame(dataframe['label'])\n\n    # Create an empty dataframe to hold the audio features\n    features  = pd.DataFrame(columns=['mfcc','pitches','magnitudes','C'])\n\n    # Loop through each audio file path in the input dataframe and compute its features\n    for index, audio_path in enumerate(dataframe['path']):\n        features.loc[index] = get_audio_features(audio_path, sampling_rate)\n\n    # Split the features into separate dataframes for each feature type\n    mfcc = features.mfcc.apply(pd.Series)\n    pit = features.pitches.apply(pd.Series)\n    mag = features.magnitudes.apply(pd.Series)\n    C = features.C.apply(pd.Series)\n\n    # Concatenate the separate feature dataframes into a single dataframe and return it with the labels\n    combined_features = pd.concat([mfcc,pit,mag,C],axis=1,ignore_index=True)\n    return combined_features, labels \n```\n\nGetting the features of audio files using librosa (Usually takes 12-15 mins to run):\n\n```{python}\ntrainfeatures, trainlabel = get_features_dataframe(train_df, sampling_rate)\ntestfeatures, testlabel = get_features_dataframe(test_df, sampling_rate)\n```\n\nFill NA values with 0:\n\n```{python}\ntrainfeatures = trainfeatures.fillna(0)\ntestfeatures = testfeatures.fillna(0)\n```\n\nConverting 2D to 1D using .ravel():\n\n```{python}\nX_train = np.array(trainfeatures)\ny_train = np.array(trainlabel).ravel()\nX_test = np.array(testfeatures)\ny_test = np.array(testlabel).ravel()\n```\n\nOne-Hot Encoding the labels:\n\n```{python}\nlb = LabelEncoder()\n\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n```\n\nChanging dimension for CNN model:\n\n```{python}\nx_traincnn =np.expand_dims(X_train, axis=2)\nx_testcnn= np.expand_dims(X_test, axis=2)\n```\n\n# CNN Model\n\n## Creating a Model\nThis defines a Sequential model for a 1D convolutional neural network (CNN). The model includes several layers of 1D convolutional layers, activation functions, dropout regularization, max pooling, and a dense layer. The model takes as input the number of MFCCs and number of frames, and outputs a probability distribution over a set of classes. The optimizer used in this model is RMSprop with a specified learning rate and decay. Overall, this code defines a CNN model for audio classification tasks, where the input is a set of features extracted from audio signals and the output is the predicted class of the audio sample.\n\n```{python}\n# Define a Sequential model for 1D convolutional neural network (CNN)\nmodel = Sequential()\n\n# Add a 1D convolutional layer with 256 filters, kernel size 5, padding same and input shape (number of MFCCs, number of frames)\nmodel.add(Conv1D(256, 5,padding='same', input_shape=(x_traincnn.shape[1],x_traincnn.shape[2])))\n\n# Add a ReLU activation function\nmodel.add(Activation('relu'))\n\n# Add a 1D convolutional layer with 128 filters, kernel size 5, padding same\nmodel.add(Conv1D(128, 5,padding='same'))\n\n# Add a ReLU activation function\nmodel.add(Activation('relu'))\n\n# Add a dropout layer with dropout rate 0.1\nmodel.add(Dropout(0.1)) #Dropout Regularization\n\n# Add a max pooling layer with pool size 8\nmodel.add(MaxPooling1D(pool_size=(8)))\n\n# Add a 1D convolutional layer with 128 filters, kernel size 5, padding same\nmodel.add(Conv1D(128, 5,padding='same',))\n\n# Add a ReLU activation function\nmodel.add(Activation('relu'))\n\n# Add a 1D convolutional layer with 128 filters, kernel size 5, padding same\nmodel.add(Conv1D(128, 5,padding='same',))\n\n# Add a ReLU activation function\nmodel.add(Activation('relu'))\n\n# Flatten the output from convolutional layers\nmodel.add(Flatten()) #Flattening the input\n\n# Add a dense layer with number of neurons equal to number of classes\nmodel.add(Dense(y_train.shape[1]))\n\n# Add a softmax activation function to output probabilities\nmodel.add(Activation('softmax'))\n\n# Define the RMSprop optimizer with learning rate and decay\nopt = tensorflow.keras.optimizers.legacy.RMSprop(learning_rate=0.00001, decay=1e-6) \n```\n\nSummary of the Sequential Model:\n\n```{python}\nmodel.summary()\n```\n\nCompile the model:\n\n```{python}\nmodel.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n```\n\n## Training and Evaluation\n\nTraining the model:\n\n```{python}\ncnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=370, validation_data=(x_testcnn, y_test), verbose=0)\n```\n\nPlotting Loss Vs Iterations:\n\n```{python}\nplt.plot(cnnhistory.history['loss'])\nplt.plot(cnnhistory.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n```\n\n## Saving the model\n\nSaving .h5 model file:\n\n```{python}\nmodel_name = 'Speech_Emotion_Recognition_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'Trained_Models')\n# Save model and weights\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)\n```\n\nSaving .json model file:\n\n```{python}\nimport json\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n```\n\n## Loading the model\n\n```{python}\n# loading json and creating model\nfrom keras.models import model_from_json\njson_file = open('model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"./Trained_Models/Speech_Emotion_Recognition_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# evaluate loaded model on test data\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(x_testcnn, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n```\n\n# Test Set Prediction\n\n### Predicting emotions on the test data\n\n```{python}\npreds = loaded_model.predict(x_testcnn, \n                         batch_size=32, \n                         verbose=1)\n\npreds=preds.argmax(axis=1)\npreds = preds.astype(int).flatten()\npredictions = (lb.inverse_transform((preds)))\npreddf = pd.DataFrame({'predictedvalues': predictions})\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactualvalues = (lb.inverse_transform((actual)))\nactualdf = pd.DataFrame({'actualvalues': actualvalues})\nfinaldf = actualdf.join(preddf)\n```\n\n## Actual v/s Predicted emotions\n\n```{python}\nfinaldf[:10]\n```\n\nSaving the predicted values in a CSV file:\n\n```{python}\nfinaldf.to_csv('Predictions.csv', index=False)\n```\n\n# Demonstration on a demo audio file\n\nLoading the demo audio file:\n\n```{python}\ndemo_audio_path = './demo_audio.wav'\nipd.Audio(demo_audio_path)\n```\n\nGetting the features of the demo audio file:\n\n```{python}\ndemo_mfcc, demo_pitch, demo_mag, demo_chrom = get_audio_features(demo_audio_path,sampling_rate)\n\nmfcc = pd.Series(demo_mfcc)\npit = pd.Series(demo_pitch)\nmag = pd.Series(demo_mag)\nC = pd.Series(demo_chrom)\ndemo_audio_features = pd.concat([mfcc,pit,mag,C],ignore_index=True)\n\ndemo_audio_features= np.expand_dims(demo_audio_features, axis=0)\ndemo_audio_features= np.expand_dims(demo_audio_features, axis=2)\n```\n\nPredicting the emotion of the demo audio file:\n\n```{python}\nlivepreds = loaded_model.predict(demo_audio_features, \n                         batch_size=32, \n                         verbose=1)\n```\n\n```{python}\nemotions=[\"Anger\",\"Disgust\",\"Fear\",\"Happy\",\"Neutral\", \"Sad\", \"Surprise\"]\nindex = livepreds.argmax(axis=1).item()\nprint(\"Emotion predicted from the model:\",emotions[index])\n```\n\n# Conclusion\n\nSpeech Emotion Recognition (SER) is an important research area in the field of signal processing and machine learning. In this project, we used the Ravdess dataset, which contains a diverse range of emotional speech recordings. Our goal was to train a deep learning model that can accurately classify the emotional state of a speaker from their speech signal. We used a combination of signal processing techniques such as MFCC, Chroma, Pitch and Magnitude to extract features from the audio signals.\n\nWe preprocessed the data by normalizing the amplitude of the signals and segmenting them into fixed-length frames. We then trained a Neural Network using the Keras API in TensorFlow to classify the emotion from the audio signal. The model architecture consisted of multiple layers of Conv1D and MaxPooling1D followed by a Flatten layer and a Dense layer with a RMSprop activation function. We used the RMSprop optimization technique to minimize the loss function and the accuracy metric to evaluate the model's performance.\n\nOur results show that the model achieved a test accuracy of 63%, which is a promising result considering the complexity of the task and the size of the dataset. We also observed that the model performed better on certain emotions, such as neutral and happy, while struggling to accurately classify others, such as disgust and surprise. This could be due to the fact that some emotions are more distinct and easily recognizable from the audio signal than others.\n\nIn conclusion, our study demonstrates the feasibility of using deep learning models for speech emotion recognition tasks. The Ravdess dataset provides a rich resource for future research in this area, with its diverse range of emotions and large number of samples. However, our study also highlights some of the challenges and limitations of this approach, such as the need for large amounts of data and the difficulty of accurately classifying certain emotions. Future work could explore alternative feature extraction techniques and model architectures to further improve the performance of speech emotion recognition systems. Additionally, the development of real-world applications for this technology could have a significant impact on fields such as psychology, human-computer interaction, and entertainment.\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"self-contained":true,"embed-resources":true,"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","theme":"cosmo","title":"<b>Emotion Recognition from Speech</b>","jupyter":"python3","page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}